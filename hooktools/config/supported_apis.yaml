# demonstrates
# At present, it is only the stage to verify whether the function is available, and the API is not yet complete

torch:
  # - is_storage
  # - is_complex
  # - is_floating_point
  # - is_nonzero
  # - set_default_dtype # have some error
  # - get_default_dtype # have some error
  # - set_default_tensor_type
  # - numel
  # - set_printoptions
  # - set_flush_denormal
  - abs
  - absolute
  - acos
  - acosh
  - add
  - addbmm
  - addcdiv
  - addcmul
  - addmm
  - addmv
  - addr
  - all
  - allclose
  - amax
  - amin
  - angle
  - any
  - arange
  - arccos
  - arccosh
  - arcsin
  - arcsinh
  - arctan
  - arctanh
  - argmax
  - argmin
  - argsort
  - asin
  - asinh
  - _assert
  - as_strided
  - as_tensor
  - atan
  - atan2
  - atanh
  - atleast_1d
  - atleast_2d
  - atleast_3d
  - baddbmm
  - bernoulli
  - bincount
  - bitwise_and
  - bitwise_not
  - bitwise_or
  - bitwise_xor
  - block_diag
  - bmm
  - broadcast_shapes
  - broadcast_tensors
  - broadcast_to
  - bucketize
  - can_cast
  - cartesian_prod
  - cat
  - cdist
  - ceil
  - chain_matmul
  - chunk
  - clamp
  - clip
  - clone
  - column_stack
  - combinations
  - complex
  - conj
  - copysign
  - cos
  - cosh
  - count_nonzero
  - cross
  - cummax
  - cummin
  - cumprod
  - cumsum
  - deg2rad
  - dequantize
  - diag
  - diag_embed
  - diagflat
  - diagonal
  - diff
  - digamma
  - dist
  - div
  - divide
  - dot
  - dstack
  - eig
  - einsum
  - empty
  - empty_like
  - empty_strided
  - enable_grad
  - eq
  - equal
  - erf
  - erfc
  - erfinv
  - exp
  - exp2
  - expm1
  - eye
  - fake_quantize_per_channel_affine
  - fake_quantize_per_tensor_affine
  - fix
  - flatten
  - flip
  - fliplr
  - flipud
  - floor
  - floor_divide
  - fmax
  - fmin
  - fmod
  - frac
  - from_numpy
  - full
  - full_like
  - gather
  - gcd
  - ge
  - geqrf
  - ger
  - get_num_interop_threads
  - get_num_threads
  - greater
  - greater_equal
  - heaviside
  - histc
  - hstack
  - hypot
  - imag
  - index_select
  - initial_seed
  - isclose
  - isfinite
  - isinf
  - isnan
  - isneginf
  - isposinf
  - isreal
  - is_tensor
  - kron
  - kthvalue
  - lcm
  - lerp
  - less
  - less_equal
  - lgamma
  - linspace
  - load
  - log
  - log10
  - log1p
  - log2
  - logaddexp
  - logaddexp2
  - logcumsumexp
  - logical_and
  - logical_not
  - logical_or
  - logical_xor
  - logit
  - logspace
  - logsumexp
  - manual_seed
  - masked_select
  - matmul
  - max
  - maximum
  - mean
  - median
  - meshgrid
  - min
  - minimum
  - mm
  - mode
  - moveaxis
  - movedim
  - msort
  - mul
  - multinomial
  - multiply
  - mv
  - mvlgamma
  - nanmedian
  - nanquantile
  - nansum
  - nan_to_num
  - narrow
  - ne
  - neg
  - negative
  - nextafter
  - no_grad
  - nonzero
  - norm
  - normal
  - not_equal
  - ones
  - ones_like
  - ormqr
  - outer
  - pinverse
  - polar
  - pow
  - prod
  - promote_types
  - qr
  - quantile
  - rad2deg
  - rand
  - randint
  - randint_like
  - rand_like
  - randn
  - randn_like
  - randperm
  - range
  - ravel
  - real
  - reciprocal
  - remainder
  - renorm
  - Repeat
  - repeat_interleave
  - reshape
  - result_type
  - roll
  - rot90
  - round
  - row_stack
  - rsqrt
  - save
  - scatter
  - scatter_add
  - searchsorted
  - set_grad_enabled
  - set_num_interop_threads
  - set_num_threads
  - set_rng_state
  - sgn
  - sigmoid
  - sign
  - signbit
  - sin
  - sinc
  - sinh
  - sort
  - split
  - sqrt
  - square
  - squeeze
  - stack
  - std
  - std_mean
  - sub
  - subtract
  - sum
  - swapaxes
  - swapdims
  - t
  - Take
  - tan
  - tanh
  - tensor
  - tensordot
  - tensor_split
  - topk
  - trace
  - transpose
  - tril
  - tril_indices
  - triu
  - triu_indices
  - true_divide
  - trunc
  - unbind
  - unique
  - unique_consecutive
  - unsqueeze
  - use_deterministic_algorithms
  - var
  - var_mean
  - view_as_complex
  - view_as_real
  - vstack
  - where
  - xlogy
  - zeros
  - zeros_like

torch.nn.modules.lazy:
  - LazyModuleMixin

torch.nn.prune:
  - PruningContainer
  - L1Unstructured
  - RandomStructured
  - LnStructured
  - CustomFromMask
  - random_unstructured
  - l1_unstructured
  - ln_structured
  - global_unstructured
  - remove
  - is_pruned

torch.nn.utils.rnn:
  - PackedSequence
  - pack_padded_sequence
  - pad_packed_sequence
  - pad_sequence
  - pack_sequence

torch.nn:
  - Conv1d
  - Conv2d
  - Conv3d
  - ConvTranspose1d
  - ConvTranspose2d
  - ConvTranspose3d
  - LazyConv1d
  - LazyConv2d
  - LazyConv3d
  - LazyConvTranspose1d
  - LazyConvTranspose2d
  - LazyConvTranspose3d
  - Unfold
  - Fold
  - MaxPool1d
  - MaxPool2d
  - MaxPool3d
  - MaxUnpool1d
  - MaxUnpool2d
  - MaxUnpool3d
  - AvgPool1d
  - AvgPool2d
  - AvgPool3d
  - ReflectionPad1d
  - ReflectionPad2d
  - ELU
  - Hardshrink
  - Hardsigmoid
  - Hardtanh
  - Hardswish
  - LeakyReLU
  - LogSigmoid
  - MultiheadAttention
  - PReLU
  - ReLU
  - RReLU
  - SELU
  - Sigmoid
  - Softplus
  - Softshrink
  - Tanh
  - Threshold
  - AdaptiveLogSoftmaxWithLoss
  - BatchNorm1d
  - BatchNorm2d
  - BatchNorm3d
  - GroupNorm
  - SyncBatchNorm
  - InstanceNorm1d
  - InstanceNorm2d
  - InstanceNorm3d
  - LayerNorm
  - RNN
  - LSTM
  - GRU
  - RNNCell
  - LSTMCell
  - GRUCell
  - TransformerEncoder
  - TransformerDecoder
  - TransformerEncoderLayer
  - TransformerDecoderLayer
  - Linear
  - LazyLinear
  - Dropout
  - Dropout2d
  - Dropout3d
  - AlphaDropout
  - Embedding
  - EmbeddingBag
  - L1Loss
  - MSELoss
  - CrossEntropyLoss
  - KLDivLoss
  - BCELoss
  - BCEWithLogitsLoss
  - MultiLabelMarginLoss
  - SmoothL1Loss
  - SoftMarginLoss
  - MultiMarginLoss
  - TripletMarginLoss
  - TripletMarginWithDistanceLoss
  - PixelShuffle
  - PixelUnshuffle
  - Upsample
  - DataParallel
  - clip_grad_norm_
  - clip_grad_value_
  - weight_norm
  - remove_weight_norm
  - spectral_norm
  - remove_spectral_norm
  - Flatten
  - Unflatten


torch.nn.functional:
  - conv1d
  - conv2d
  - conv3d
  - conv_transpose1d
  - conv_transpose2d
  - conv_transpose3d
  - unfold
  - fold
  - avg_pool1d
  - avg_pool2d
  - avg_pool3d
  - max_pool1d
  - max_pool2d
  - max_pool3d
  - max_unpool1d
  - max_unpool2d
  - max_unpool3d
  - lp_pool1d
  - lp_pool2d
  - adaptive_max_pool1d
  - adaptive_max_pool2d
  - adaptive_max_pool3d
  - adaptive_avg_pool1d
  - adaptive_avg_pool2d
  - adaptive_avg_pool3d
  - threshold
  - relu
  - hardtanh
  - hardswish
  - relu6
  - elu
  - elu_
  - selu
  - celu
  - leaky_relu
  - leaky_relu_
  - prelu
  - rrelu
  - rrelu_
  - glu
  - gelu
  - logsigmoid
  - hardshrink
  - tanhshrink
  - softplus
  - softmin
  - softmax
  - softshrink
  - gumbel_softmax
  - log_softmax
  - tanh
  - sigmoid
  - hardsigmoid
  - silu
  - normalize
  - linear
  - dropout
  - embedding
  - embedding_bag
  - one_hot
  - binary_cross_entropy
  - binary_cross_entropy_with_logits
  - poisson_nll_loss
  - cosine_embedding_loss
  - cross_entropy
  - ctc_loss
  - hinge_embedding_loss
  - kl_div
  - l1_loss
  - mse_loss
  - margin_ranking_loss
  - multilabel_margin_loss
  - multilabel_soft_margin_loss
  - multi_margin_loss
  - nll_loss
  - smooth_l1_loss
  - soft_margin_loss
  - triplet_margin_loss
  - triplet_margin_with_distance_loss
  - pad
  - interpolate
  - grid_sample
  - affine_grid

torch.Tensor:
  - abs
  - abs_
  - absolute
  - absolute_
  - acos
  - acos_
  - arccos
  - arccos_
  - add
  - add_
  - addbmm
  - addbmm_
  - addcdiv
  - addcdiv_
  - addcmul
  - addcmul_
  - addmm
  - addmm_
  - sspaddmm
  - addmv
  - addmv_
  - addr
  - addr_
  - allclose
  - amax
  - amin
  - angle
  - argmax
  - argmin
  - argsort
  - asin
  - asin_
  - arcsin
  - arcsin_
  - atan
  - atan_
  - arctan
  - arctan_
  - atan2
  - atan2_
  - all
  - any
  - backward
  - baddbmm
  - baddbmm_
  - bernoulli
  - bernoulli_
  - bincount
  - bitwise_not
  - bitwise_not_
  - bitwise_and
  - bitwise_and_
  - bitwise_or
  - bitwise_or_
  - bitwise_xor
  - bitwise_xor_
  - bmm
  - bool
  - byte
  - broadcast_to
  - cauchy_
  - ceil
  - ceil_
  - char
  - cholesky
  - cholesky_inverse
  - cholesky_solve
  - chunk
  - clamp
  - clamp_
  - clip
  - clip_
  - clone
  - copy_
  - conj
  - copysign
  - copysign_
  - cos
  - cos_
  - cosh
  - cosh_
  - count_nonzero
  - acosh
  - acosh_
  - acosh
  - acosh_
  - cpu
  - cross
  - cuda
  - logcumsumexp
  - cummax
  - cummin
  - cumprod
  - cumprod_
  - cumsum
  - cumsum_
  - data_ptr
  - deg2rad
  - dequantize
  - diag
  - diag_embed
  - diagflat
  - diagonal
  - fill_diagonal_
  - diff
  - digamma
  - digamma_
  - dim
  - dist
  - div
  - div_
  - divide
  - divide_
  - dot
  - eig
  - element_size
  - eq
  - eq_
  - equal
  - erf
  - erf_
  - erfc
  - erfc_
  - erfinv
  - erfinv_
  - exp
  - exp_
  - expm1
  - expm1_
  - expand
  - expand_as
  - exponential_
  - fix
  - fix_
  - fill_
  - flatten
  - flip
  - fliplr
  - flipud
  - float
  - float_power
  - float_power_
  - floor
  - floor_
  - floor_divide
  - floor_divide_
  - fmod
  - fmod_
  - frac
  - frac_
  - gather
  - gcd
  - gcd_
  - ge
  - ge_
  - greater_equal
  - greater_equal_
  - geometric_
  - geqrf
  - gt
  - gt_
  - greater
  - greater_
  - half
  - hardshrink
  - heaviside
  - histc
  - hypot
  - hypot_
  - index_add_
  - index_add
  - index_copy_
  - index_copy
  - index_fill_
  - index_fill
  - index_put_
  - index_put
  - index_select
  - int
  - int_repr
  - isclose
  - isfinite
  - isinf
  - isposinf
  - isneginf
  - isnan
  - is_set_to
  - is_signed
  - isreal
  - item
  - kthvalue
  - lcm
  - lcm_
  - le
  - le_
  - less_equal
  - less_equal_
  - lerp
  - lerp_
  - lgamma
  - lgamma_
  - log
  - log_
  - logdet
  - log10
  - log10_
  - log1p
  - log1p_
  - log2
  - log2_
  - logaddexp
  - logaddexp2
  - logsumexp
  - logical_and
  - logical_and_
  - logical_not
  - logical_not_
  - logical_or
  - logical_or_
  - logical_xor
  - logical_xor_
  - logit
  - logit_
  - long
  - lt
  - lt_
  - lt
  - less_
  - map_
  - def callable
  - masked_fill
  - masked_select
  - matmul
  - max
  - maximum
  - mean
  - median
  - nanmedian
  - min
  - minimum
  - mm
  - mode
  - movedim
  - moveaxis
  - msort
  - mul
  - mul_
  - multiply
  - multiply_
  - multinomial
  - mv
  - mvlgamma
  - mvlgamma_
  - nansum
  - narrow
  - narrow_copy
  - ndimension
  - nan_to_num
  - nan_to_num_
  - ne
  - ne_
  - not_equal
  - not_equal_
  - neg
  - neg_
  - negative
  - negative_
  - nelement
  - nextafter
  - nextafter_
  - nonzero
  - norm
  - normal_
  - ormqr
  - outer
  - permute
  - pin_memory
  - pinverse
  - pow
  - pow_
  - prod
  - put_
  - quantile
  - nanquantile
  - q_scale
  - q_zero_point
  - q_per_channel_scales
  - q_per_channel_zero_points
  - q_per_channel_axis
  - rad2deg
  - random_
  - ravel
  - reciprocal
  - reciprocal_
  - record_stream
  - register_hook
  - hook
  - remainder
  - remainder_
  - renorm
  - renorm_
  - repeat
  - repeat_interleave
  - requires_grad_
  - reshape
  - reshape_as
  - resize_
  - resize_as_
  - roll
  - rot90
  - round
  - round_
  - rsqrt
  - rsqrt_
  - scatter
  - scatter_
  - scatter_add
  - select
  - set_
  - short
  - sigmoid
  - sigmoid_
  - sign
  - sign_
  - signbit
  - sgn
  - sgn_
  - sin
  - sin_
  - sinc
  - sinc_
  - sinh
  - sinh_
  - asinh
  - asinh_
  - arcsinh
  - arcsinh_
  - size
  - solve
  - sqrt
  - sqrt_
  - square
  - square_
  - squeeze
  - squeeze_
  - storage
  - storage_offset
  - storage_type
  - stride â†’ tuple or int
  - sub
  - sub_
  - subtract
  - subtract_
  - sum
  - sum_to_size
  - svd
  - swapaxes
  - swapdims
  - t
  - t_
  - tensor_split
  - tile
  - to
  - to
  - to
  - take
  - tan
  - tan_
  - tanh
  - tanh_
  - atanh
  - atanh_
  - arctanh
  - arctanh_
  - tolist
  - trace
  - transpose
  - transpose_
  - tril
  - tril_
  - triu
  - triu_
  - true_divide
  - true_divide_
  - trunc
  - trunc_
  - type_as
  - unfold
  - uniform_
  - unsqueeze
  - unsqueeze_
  - values
  - var
  - vdot
  - view
  - view
  - view_as
  - where
  - xlogy
  - xlogy_
  - zero_

torch.autograd:
  - backward
  - grad
  - functional.jacobian
  - functional.hessian
  - functional.vjp
  - functional.jvp
  - functional.vhp
  - functional.hvp
  - torch.Tensor
  - retain_grad
  - Function
  - function._ContextMethodMixin
  - mark_dirty
  - mark_non_differentiable
  - save_for_backward
  - set_materialize_grads
  - profiler.profile
  - use_cuda
  - record_shapes
  - with_flops
  - profile_memory
  - with_stack
  - use_cpu
  - export_chrome_trace
  - key_averages
  - detect_anomaly

torch.cuda:
  - current_device
  - current_stream
  - default_stream
  - device
  - device_count
  - get_device_capability
  - get_device_name
  - get_device_properties
  - is_available
  - set_device
  - stream
  - synchronize
  - get_rng_state_all
  - set_rng_state_all
  - manual_seed
  - initial_seed
  - comm.broadcast
  - comm.broadcast_coalesced
  - comm.reduce_add
  - comm.scatter
  - comm.gather
  - Stream
  - query
  - record_event
  - synchronize
  - wait_event
  - wait_stream
  - Event
  - elapsed_time
  - record
  - wait
  - empty_cache
  - memory_stats
  - memory_allocated
  - max_memory_allocated
  - memory_reserved
  - max_memory_reserved
  - set_per_process_memory_fraction
  - max_memory_cached

torch.backends:
  - cudnn.is_available
  - mkl.is_available
  - mkldnn.is_available
  - openmp.is_available

torch.futures:
  - Future
  - collect_all
  - wait_all

torch.fx:
  - symbolic_trace
  - wrap
  - GraphModule
  - Graph
  - Node
  - Tracer
  - Proxy
  - Interpreter
  - Transformer

torch.overrides:
  - get_overridable_functions
  - handle_torch_function
  - has_torch_function
  - is_tensor_method_or_property
  - wrap_torch_function

torch.profiler:
  - profile
	- schedule
	- tensorboard_trace_handler

torch.nn.init:
  - calculate_gain
  - uniform_
  - normal_
  - constant_
  - ones_
  - zeros_
  - eye_
  - dirac_
  - xavier_uniform_
  - xavier_normal_
  - kaiming_uniform_
  - kaiming_normal_
  - orthogonal_

torch.optim:
  - Adadelta
  - Adam
  - SGD
  - lr_scheduler.LambdaLR
  - lr_scheduler.MultiplicativeLR
  - lr_scheduler.StepLR
  - lr_scheduler.MultiStepLR
  - lr_scheduler.ExponentialLR
  - lr_scheduler.CosineAnnealingLR
  - lr_scheduler.ReduceLROnPlateau
  - lr_scheduler.CyclicLR
  - lr_scheduler.OneCycleLR
  - lr_scheduler.CosineAnnealingWarmRestarts

torch.random:
  - fork_rng
  - initial_seed
  - manual_seed

torch.Storage:
 - fill_
 - float
 - from_buffer
 - from_file
 - get_device
 - half
 - int
 - is_cuda
 - is_pinned
 - is_shared
 - is_sparse
 - long
 - new
 - pin_memory
 - resize_
 - share_memory_
 - short
 - size
 - tolist
 - type

torch.utils.benchmark:
  - Timer
  - Measurement
  - CallgrindStats
  - FunctionCounts


torch.utils.checkpoint:
  - checkpoint
  - checkpoint_sequential

torch.utils.cpp_extension:
	- CppExtension
  - CUDAExtension
  - BuildExtension
  - load
  - load_inline
  - include_paths
  - check_compiler_abi_compatibility

torch.utils.data:
	- DataLoader
  - Dataset
  - IterableDataset
  - TensorDataset
  - ConcatDataset
  - ChainDataset
  - BufferedShuffleDataset
  - get_worker_info
  - random_split
  - Sampler
  - SequentialSampler
  - RandomSampler
  - SubsetRandomSampler
  - WeightedRandomSampler
  - BatchSampler
  - distributed.DistributedSampler


torch.view:
  - as_strided
  - detach
  - diagonal
  - expand
  - expand_as
  - movedim
  - narrow
  - permute
  - select
  - squeeze
  - transpose
  - t
  - T
  - real
  - imag
  - view_as_real
  - view_as_imag
  - unflatten
  - unfold
  - unsqueeze
  - view
  - view_as
  - unbind
  - split
  - split_with_sizes
  - swapaxes
  - swapdims
  - chunk
  - indices
  - values